# -*- coding: utf-8 -*-
"""cpe428_hw5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KHBQFYdUeS27esiK8OmD0E1BoxP9yjPY

### CPE 428 Homework 5: Medical Image Classification ###

JeanReno Racines 

Professor Ventura

11-22-2020

## Part I: Create the convolutional neural network (5 points) ##
Set up a convolutional neural network for binary classification on the images.
"""

import keras
from keras.models import Model
from keras.layers import *

import numpy as np
import cv2 as cv
import matplotlib as mpl
from matplotlib import pyplot as plt

from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people
from sklearn.model_selection import train_test_split

"""Use a Keras utility function to load the dataset.

As a dataset we will use the Kaggle "Breast Histopathology Images" dataset which contains 198,748 non-IDC images and 78,786 IDC images. Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide.  
The images are size 50x50 but you should crop them to 48x48 to make it easier to set up the CNN.
"""

from keras.utils import get_file
x_train_path = get_file('idc_train.h5','https://storage.googleapis.com/cpe428-fall2020-datasets/idc_train.h5')
x_test_path = get_file('idc_test.h5','https://storage.googleapis.com/cpe428-fall2020-datasets/idc_test.h5')
# x_val_path = get_file('idc_val.h5','https://storage.googleapis.com/cpe428-fall2020-datasets/idc_val.h5')

"""**Preprocessing**

Read the data from HDF5 file into Numpy arrays and crop to 48x48. 'X' represents the images while 'y' represents the labels.
"""

import h5py as h5

# The training dataset
with h5.File(x_train_path,'r') as f:
  # x_train = f['X'][:111464,1:49,1:49]
  # y_train = f['y'][:111464]
  x_train = f['X'][:,1:49,1:49]  
  y_train = f['y'][:]

# Testing  dataset
with h5.File(x_test_path,'r') as f:
  x_test = f['X'][:,1:49,1:49]
  y_test = f['y'][:]

# This the validation dataset which we pull from the training set
# with h5.File(x_train_path,'r') as f:
#   x_val = f['X'][111464:,1:49,1:49]
#   y_val = f['y'][111464:]

"""Print some info about the dataset:"""

print('Training Images shape:',x_train.shape)
print('Training Labels shape:',y_train.shape)
# print('Validation Images shape:',x_val.shape)
# print('Validation Labels shape:',y_val.shape)
print('Testing Images shape:',x_test.shape)
print('Testing Labels shape:',y_test.shape)
print('First 100 training labels:',y_train[:100])
print('First 100 testing labels:',y_test[:100])
nclasses = np.max(y_train)+1
print('Number of classes:',nclasses)

"""Show a few images from the dataset."""

import numpy as np
from matplotlib import pyplot as plt
for i in range(5):
  plt.imshow(np.squeeze(x_train[i]))
  plt.title(y_train[i])
  plt.show()

"""Compute the mean intensity of the training data and subtract it from all images.  Neural networks train more effectively when the data is centered. The average of training set is used since the model must have have no knowledge of the test set."""

training_mean = np.uint8(np.mean(x_train))
x_train -= training_mean
# x_val -= training_mean
x_test -= training_mean

"""**Model design**

Build the CNN model using Keras layers.

* Create a VGG-like network with blocks of convolution, ReLU, and max pooling.
* At the end of the network, flatten to a vector and use dense layers.
* We use a softmax activation at the end to ensure the output probabilities sum to 1.
* Zero padding ensures the output is the same size as the input
"""

act = 'relu'                        # Removes negative values
layers_per_block = 2                # 2 convolution per block
num_channels = 64
 
# Regularizer pulls back on model strength
#    - Prevents weights from getting too big
#    - Helps prevent overfitting
reg = keras.regularizers.l2(.1)  
 
# Using this function to make it easier/cleaner to add more blocks
def build_block(x, num_channels):
  for i in range(layers_per_block):
    x = Conv2D(num_channels, 3, 
               padding='same', 
               kernel_regularizer=reg)(x) 
    x = Activation(act)(x)    # Non-linear activation
  x = MaxPooling2D(2,2)(x)    # Output max over a sliding window
                              # 2x2 with stride 2; downsamples 
  return x
 
x_in = Input(x_train.shape[1:])     # size is 48x48
 
# First block
x = build_block(x_in, 64)           # size is 24x24
 
# Second block
x = build_block(x, 128)             # size is 12x12
 
# Third block
x = build_block(x, 256)             # size is 6x6
 
# Flatten to a vector
#   - Make it so that layer operation is a matrix-vector multiply 
#     rather than a convolution
x = Flatten()(x)              
x = Dense(48)(x)              # Fully-connected layer
x = Activation(act)(x)
x = Dense(48)(x)              # Fully-connected layer
x = Activation(act)(x)
x = Dense(nclasses)(x)        # 2 outputs, 1 for each class
x = Activation('softmax')(x)  # Normalizes probabilities to 1

model = Model(inputs=x_in,outputs=x)
model.summary()

"""Compile the model: set the loss function, optimizer, and metrics to monitor during training.

* sparse_categorical_crossentropy: appropriate for classification problems and integer labels.
* Adam: popular weight optimizer to eliminate the need for manual tuning (3e-4 is the magic number).
* sparse_categorical_accuracy: tells us the proportion of correctly classified images after each epoch.
"""

model.compile(loss='sparse_categorical_crossentropy',
              optimizer=keras.optimizers.Adam(lr=3e-4),
              metrics=['sparse_categorical_accuracy'])

"""**Before** training, the model with randomly initialized weights is so bad."""

results = model.evaluate(x_train,y_train,verbose=0)
print('Training Accuracy: %.2f %%'%(results[1]*100))
results = model.evaluate(x_test,y_test,verbose=0)
print('Testing Accuracy: %.2f %%'%(results[1]*100))

"""## Part II: Train the neural network. (5 points) ##

**Training the model**

Train the model on the training data and plot the training and validation loss and accuracy over time.

Show some test images with their prediction (cat or dog) and ground truth label.

* shuffle=True: tells Keras to randomly shuffle the data before each epoch.
* 1 epoch = 1 pass over the data
* batch size: how many images are shown at once to the network.
* validation_split=0.1: tells Keras to withold 10% of the training data for validation.  We only look at the metrics on this data, we don't use it to actually update the weights of the model.
"""

history = model.fit(x_train,y_train,
                    shuffle=True,
                    epochs=200,
                    batch_size=1024,
                    validation_split=0.1)

"""## Part III: Test the neural network. (5 points) ##

Calculate the train and test accuracy.

Is the test accuracy close to the training accuracy?  

Does the model seem to be "overfitting" (the validation loss stops decreasing and starts increasing at some point)?

What happens to the number of parameters in the model if you use fewer blocks or more blocks?

What happens to your training and testing accuracy if you use fewer or more blocks?  Why?

What happens if you increase or decrease the number of channels in the layers?  

Does the performance increase or decrease?  Why?
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training Loss','Validation Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

plt.plot(history.history['sparse_categorical_accuracy'])
plt.plot(history.history['val_sparse_categorical_accuracy'])
plt.legend(['Training Accuracy','Validation Accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()

"""Calculate the final accuracy on the training and test set.  """

results = model.evaluate(x_train,y_train,verbose=0)
print('Training Accuracy: %.2f %%'%(results[1]*100))
results = model.evaluate(x_test,y_test,verbose=0)
print('Testing Accuracy: %.2f %%'%(results[1]*100))

"""model.predict() returns the output of the network.  For each image we get a row of nclasses probability values which add up to 1.

Bright line on the diagonal if the classification was perfect.
"""

preds = model.predict(x_test,verbose=0)
print('output shape: ',preds.shape)
print(preds[1])
plt.imshow(preds)
plt.show()

"""Show each test image with the correct and predicted label."""

for im,label,pred in zip(x_test,y_test,preds):
  predlabel = np.argmax(pred)
  imout = ((np.squeeze(im)+training_mean)*255).astype('uint8')
  plt.imshow(imout)
  plt.title('Label: %d / Pred: %d'%(label,predlabel))
  plt.show()